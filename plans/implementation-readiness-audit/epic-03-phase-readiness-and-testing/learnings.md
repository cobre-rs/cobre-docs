# Epic 03 Learnings: Implementation Phase Readiness and Test Adequacy

## Patterns Established

- **Five-dimension per-phase readiness template**: the phase assessment template (spec reading list completeness, testable intermediate achievability, gap impact, implicit knowledge gaps, cross-phase dependency risks) produces a structured verdict (READY / CONDITIONALLY READY / NOT READY) that directly feeds the go/no-go verdict in the final epic. The 5 dimensions are not all equal in weight: the gap impact dimension (consuming triage classifications from epic-02) is the most mechanical; the implicit knowledge gaps dimension requires domain expertise and cannot be automated. See `plans/implementation-readiness-audit/epic-03-phase-readiness-and-testing/report-012-phase-1-4-readiness.md` sections 2-5 and `report-013-phase-5-8-readiness.md` sections 2-5 for the per-phase application.

- **Pre-coding conditions inventory as the primary epic output**: the most actionable artifact in each phase assessment report is the consolidated pre-coding conditions inventory (section 7 of report-012, section 8 of report-013), not the individual phase verdicts. The inventory assigns effort estimates, sequencing constraints, and resolution deadlines to each condition, enabling the team to schedule spec work against the implementation timeline. Future phase assessment tickets should explicitly require this inventory format rather than inferring it from individual phase sections.

- **Six-dimension testing spec adequacy framework**: the testing spec audit (report-014) operates on six dimensions: structural compliance, conformance test coverage, error path coverage, cross-backend equivalence, finite-difference sensitivity checks, and test count adequacy. Dimensions 4 and 5 apply only where semantically relevant (multi-backend traits and dual/gradient postconditions respectively), making the audit self-annotating with N/A cells. This framework verified 247 named tests across 7 specs without re-reading the trait specs in detail -- the coverage table format (method, variants tested, test count, boundary cases) provides sufficient evidence for the adequacy determination. See `report-014-testing-adequacy.md` section 2 for all 7 per-spec assessments.

- **Simulation pipeline deep-dive as a required sub-section for the most PARTIAL+MISSING subsystem**: when a crate or subsystem exceeds the 30% PARTIAL+MISSING threshold (identified in epic-01 crate audits), the phase readiness assessment must include a dedicated per-capability enumeration rather than a summary completeness percentage. The simulation pipeline deep-dive (report-013 section 6) classifies all 12 capabilities individually and reveals that 8 are COMPLETE but 3 MISSING items all concern the same root cause (output API formalization). Without the deep-dive, the `33% PARTIAL+MISSING` figure suggests scattered deficiencies; with it, the gap becomes a single focused spec task. See `report-013-phase-5-8-readiness.md` section 6.

## Architectural Decisions

- **Phase 7 is the only NOT READY phase in the 8-phase build sequence**: the NOT READY verdict rests on three interdependent gaps -- simulation orchestrator function signature (F-012 in report-005), `SimulationScenarioResult` type (GAP-030), and output writer API (GAP-020). These are not independently fixable: the result type depends on understanding the streaming architecture; the writer API depends on the result type for its method signatures; the orchestrator signature depends on both. The correct resolution order is (1) define `SimulationScenarioResult`, (2) define the writer API, (3) define the orchestrator signature -- all in `simulation-architecture.md`. All three should be resolved during Phase 6 implementation before Phase 7 begins. See `report-013-phase-5-8-readiness.md` section 4.

- **Phase 6 is READY with zero pre-coding conditions despite 8 Resolve-During-Phase gaps**: the high gap count for Phase 6 (cobre-sddp training, 8 gaps) does not produce conditions because all 8 gaps are internal to cobre-sddp and do not affect crate boundary types. The cumulative risk of 8 local decisions was assessed as low because each gap has a documented default or recommendation. This decision establishes the principle that gap count is not the correct readiness metric -- gap classification (Resolve-Before-Coding vs Resolve-During-Phase) is the correct metric. See `report-013-phase-5-8-readiness.md` section 3.3.

- **Total pre-coding effort is 9-15 working days across all 8 phases**: report-012 estimates 5-9 days for Phases 1-4, report-013 estimates 4-6 days for Phases 5-8. The estimates use conservative ranges because most conditions involve spec authoring (entity struct definitions, API design) rather than pure mechanical edits. The Phase 7 resolution (GAP-020 + simulation types) is the most effort-intensive item (2-3 days) and has the highest integration risk because it designs a new crate boundary. See report-012 section 7.5 and report-013 section 7 summary rows.

- **Testing spec corpus is ADEQUATE across all 7 specs with no INADEQUATE verdict**: the testing specs exhibit strong structural consistency -- all 7 comply with the shared-fixture-at-top, naming-convention-paragraph, and convention-blockquote-absent requirements from CLAUDE.md. The two ADEQUATE WITH GAPS verdicts are both minor: `solver-interface-testing.md` is missing tests for the 10th trait method (`patch_col_bounds`) and has 8 stale method name occurrences; `backend-testing.md` deviates from the shared-fixture pattern because communication tests require rank-specific inline data. Neither gap blocks implementation. See `report-014-testing-adequacy.md` section 4 summary table.

## Files and Structures Created

- `plans/implementation-readiness-audit/epic-03-phase-readiness-and-testing/report-012-phase-1-4-readiness.md` -- Phase 1-4 readiness assessment (7 sections). Per-phase analysis for cobre-core (47% COMPLETE, CONDITIONALLY READY), cobre-io (GAP-029 blocker, CONDITIONALLY READY), ferrompi + cobre-solver (naming/ownership conditions, CONDITIONALLY READY), and cobre-comm (fully specified, READY). Cross-phase summary table and 8 pre-coding conditions with effort estimates totaling 5-9 working days.

- `plans/implementation-readiness-audit/epic-03-phase-readiness-and-testing/report-013-phase-5-8-readiness.md` -- Phase 5-8 readiness assessment (8 sections). Per-phase analysis for cobre-stochastic (GAP-023 blocker, CONDITIONALLY READY), cobre-sddp training (0 pre-coding blockers, READY), cobre-sddp simulation + cobre-io (3 MISSING API elements, NOT READY), and cobre-cli (cross-spec naming fixes, CONDITIONALLY READY). Includes simulation pipeline deep-dive (12 capabilities, 8 COMPLETE, 1 PARTIAL, 3 MISSING) and 7 pre-coding conditions with resolution scheduling.

- `plans/implementation-readiness-audit/epic-03-phase-readiness-and-testing/report-014-testing-adequacy.md` -- Testing spec adequacy audit (5 sections). Per-spec assessments for all 7 testing specs against 6 adequacy dimensions. 247 total named tests verified. Summary table with method coverage percentages. Missing tests inventory: 3 HIGH priority (`patch_col_bounds` coverage for HiGHS, CLP, cross-solver), 2 MEDIUM (stale naming corrections), 4 LOW (difficult-to-trigger error variants and backend-testing structural).

## Conventions Adopted

- **Pre-coding conditions inventory format**: each entry in the inventory must specify: condition ID, source (gap ID or report finding reference), description (one sentence), effort estimate (hours or days), and dependency (what must be resolved or read first). The Phase 8 conditions table in report-013 section 8.1 adds two additional columns (Earliest Resolution and Latest Resolution) that are valuable for scheduling -- this extended format should be used in all future phase assessment reports.

- **NOT READY verdict requires enumeration of the minimum blocking set**: a NOT READY verdict is only credible if the report identifies the minimum set of spec gaps that, if resolved, would change the verdict to CONDITIONALLY READY or READY. For Phase 7, this minimum set is three items (simulation result type, orchestrator signature, writer API). A single broad statement ("the output layer is unspecified") is insufficient; the verdict must name every missing API element that crosses a crate boundary. See `report-013-phase-5-8-readiness.md` section 4.4 for the enumeration methodology.

- **Resolution scheduling batches**: when multiple pre-coding conditions exist across multiple phases, group them into resolution batches ordered by dependency deadline: Batch 1 items have no dependencies and can be resolved immediately (before any coding begins), Batch 2 items depend on earlier implementation work and should be resolved inline with the phase that unblocks them. Report-013 section 8.2 established this two-batch pattern for the Phase 5-8 conditions. The key insight is that Batch 2 items (GAP-020 and simulation types) are best designed after Phase 6 training implementation is underway, because the training pipeline patterns directly inform the simulation API shape.

- **Deferred test variants are not gaps**: the testing spec adequacy audit explicitly distinguishes between (a) tests for minimal viable solver variants (Expectation, HiGHS, Finite, InSample, Level-1, all 5 stopping rules) and (b) deferred variants (CVaR, CLP, Cyclic, External, Dominated). Only the former are checked against the spec; missing tests for deferred variants are correctly excluded from the missing tests inventory. This convention prevents the audit from inflating gap counts with post-MVP items.

## Surprises and Deviations

- **Phase 4 (cobre-comm) is the only fully READY phase in Phases 1-4**: this was unexpected given that cobre-comm's spec completeness is 77% (43/56 items), lower than ferrompi (96%) and cobre-solver (79%). The READY verdict reflects that all cobre-comm findings are Resolve-During-Phase strategy choices (boxing for SharedMemoryProvider enum dispatch, CommData bounds narrowing) that an experienced Rust developer resolves inline. The completeness percentage understates readiness because many PARTIAL items are strategy choices rather than missing specifications.

- **Phase 3 conditions are more numerous than Phase 1 conditions despite higher spec completeness**: Phase 3 has 4 conditions (compared to Phase 1's 3) even though ferrompi is 96% COMPLETE and cobre-solver is 79% COMPLETE. The explanation is that Phase 3 conditions are about cross-crate naming consistency (stale API names, method rename) and ownership clarity (`StageTemplate`/`StageIndexer` crate boundary), not about missing spec content. High completeness percentage does not guarantee zero conditions -- the conditions audit caught issues that the completeness audit missed because it checks naming consistency, not just presence/absence.

- **The testing specs require no new tests for 5 of 7 specs**: the expectation entering the audit was that several testing specs would be found inadequate, given that the spec-readiness plan had focused on trait specs. In practice, all 5 architecture testing specs for pure-query traits (risk measure, horizon mode, sampling scheme, cut selection, stopping rule) were rated ADEQUATE with zero missing tests for the minimal viable solver scope. The testing specs were authored with sufficient discipline during the spec-readiness plan that a subsequent adequacy audit found only the pre-identified `solver-interface-testing.md` naming issue.

- **The `patch_col_bounds` gap was not in any prior report**: none of the epic-01, epic-02, or gap inventory reports flagged that the 10th `SolverInterface` method has zero test coverage. The testing spec adequacy audit discovered this independently by cross-referencing the trait method list against the test coverage table. This demonstrates that the adequacy audit methodology (build a coverage matrix from trait methods vs test names) catches gaps that prior audits miss because prior audits read the testing spec in isolation rather than comparing it method-by-method against the trait spec.

- **backend-testing.md structural deviation is justified**: the shared-fixture requirement from CLAUDE.md was written with architecture testing specs in mind (where inputs are fixed mathematical objects like penalty tables or LP data). Communication tests require rank-specific inline data because each rank sends different values in an allgatherv test. The backend-testing.md ADEQUATE WITH GAPS verdict correctly identifies the structural deviation as a partial compliance rather than a violation, because the deviation has a semantic justification that doesn't exist for architecture specs.

## Aggregate Statistics

- Phases assessed: 8 (Phases 1-8 of implementation-ordering.md)
- Phase verdicts: 3 CONDITIONALLY READY (Phases 1, 2, 3), 1 READY (Phase 4), 1 CONDITIONALLY READY (Phase 5), 1 READY (Phase 6), 1 NOT READY (Phase 7), 1 CONDITIONALLY READY (Phase 8)
- Total pre-coding conditions: 15 (8 for Phases 1-4, 7 for Phases 5-8)
- Total pre-coding effort: 9-15 working days
- Phase 7 exclusive blocker: GAP-020 (output writer API) + 2 derived missing types; estimated 2-3 days
- Testing specs audited: 7
- Total named tests verified: 247
- Adequacy verdicts: ADEQUATE (5), ADEQUATE WITH GAPS (2), INADEQUATE (0)
- Structural compliance violations: 0 across all 7 specs
- Missing tests (HIGH priority): 3 (all for `patch_col_bounds` method)
- Missing tests (MEDIUM priority): 2 (stale name corrections for `patch_row_bounds`)
- Missing tests (LOW priority): 4 (hard-to-trigger error variants and structural fixture)

## Recommendations for Future Epics

- **Epic-04 verdict synthesis should focus on GAP-020 as the highest-priority pre-coding condition**: of all 15 conditions across 8 phases, GAP-020 is the only one that (a) requires new API design rather than struct sketching, (b) defines a new crate boundary between two crates in the critical path, and (c) has an explicit resolution window (during Phase 6). The verdict should present this as the single most important spec work item before coding begins. See `report-013-phase-5-8-readiness.md` section 4.4 for the complete list of 9 missing API elements that constitute GAP-020.

- **Epic-04 should include a data model end-to-end trace that targets Phase 7 types specifically**: the `SimulationScenarioResult` type (MISSING) and simulation orchestrator signature (MISSING) are the two types that must be defined before Phase 7 can begin. The data model trace in ticket-015 should verify that these types, once defined, are consistent with the existing `output-infrastructure.md` streaming architecture and `output-schemas.md` entity schemas. The trace should follow the data flow: per-stage LP solve -> state vector extraction -> result struct assembly -> bounded channel -> I/O thread -> Parquet write.

- **The 3 HIGH-priority missing tests in `solver-interface-testing.md` should be added before Phase 3 implementation begins**: `test_solver_highs_patch_col_bounds_basic`, `test_solver_clp_patch_col_bounds_basic`, and `test_solver_cross_patch_col_bounds_agreement` are needed because `patch_col_bounds` is a Phase 3 deliverable (cobre-solver). An implementer writing Phase 3 without these tests will have no conformance target for the 10th trait method. The tests are straightforward to specify: load the existing LP fixture, patch column 2 upper bound from 8.0 to 4.0, solve, verify $x_2 \leq 4.0$ and objective changes. See `report-014-testing-adequacy.md` section 5 for the exact test names.

- **The 8 stale `patch_rhs_bounds` occurrences in `solver-interface-testing.md` must be resolved before Phase 3**: this was flagged in epic-01 (report-004 F-007), flagged again in epic-02 (finding F3), and confirmed in epic-03 (report-014 section 2.6). Three consecutive audits have identified this defect without it being fixed. If it persists into Phase 3, the implementer will encounter contradictory method names between the trait spec (`patch_row_bounds`) and the testing spec (`patch_rhs_bounds`). The fix is 8 find-and-replace substitutions plus a new entry for `patch_col_bounds` in the lifecycle test. See `report-014-testing-adequacy.md` section 5 MEDIUM priority items.

- **Phase 8 cross-spec fixes (SLURM invocation pattern, config field name, `training.enabled`) are independent and low-effort**: all three Phase 8 conditions from `report-013-phase-5-8-readiness.md` section 8.1 (conditions #4, #5, #6) can be resolved in a single day by one developer with no dependencies. The SLURM scripts must replace `cobre train --config /path/to/config.json` with `cobre run /path/to/case_directory`; the config field name must be standardized to `training.forward_passes` across 4 spec files; `training.enabled` must be added to `configuration-reference.md`. These should be batched into a single low-effort spec-fix ticket rather than deferred to Phase 8 implementation.
