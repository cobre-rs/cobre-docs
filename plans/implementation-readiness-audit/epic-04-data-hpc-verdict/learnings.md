# Epic 04 Learnings: Data Model Traceability, HPC Correctness, and Readiness Verdict

## Patterns Established

- **End-to-end chain tracing as a distinct audit dimension**: the data model traceability audit (report-015) establishes a four-verdict chain classification (COMPLETE / PARTIAL / SCHEMA ONLY / MISSING) that operates independently of the per-crate completeness audit. A chain verdict aggregates across three links -- schema spec, pipeline function, in-memory type -- and reveals asymmetries that crate completeness percentages hide. The input/output asymmetry (3 COMPLETE + 1 PARTIAL input vs 0 COMPLETE + 2 PARTIAL + 3 SCHEMA ONLY output) in report-015 sections 2 and 3 is the clearest single picture of what remains before Phase 7 coding can begin.

- **Systemic gap diagnosis from chain convergence**: when every chain in a direction (input or output) converges on the same missing link, the finding is systemic rather than distributed. All 4 output chains blocked by GAP-020 means that one resolution action (defining the output writer API) unblocks the entire output side simultaneously. This convergence pattern -- discovered by the chain trace but invisible to the crate audit -- is the strongest argument for scheduling GAP-020 as a single focused spec task rather than four separate repairs. See `plans/implementation-readiness-audit/epic-04-data-hpc-verdict/report-015-data-traceability.md` section 5, Finding 1.

- **HPC spec correctness audit as four orthogonal dimensions**: the HPC correctness audit (report-016) audits four independent dimensions -- MPI collective parameter specificity, threading model consistency, NUMA/cache actionability, ferrompi API surface match -- and assigns a separate verdict per dimension. This avoids the averaging effect of a single overall score. The strongest dimension (MPI parameter specificity: SUFFICIENT WITH GAPS, 0 blocking findings) and the weakest (ferrompi API surface: SUFFICIENT WITH GAPS, 4 stale name occurrences) have very different remediation profiles. See `plans/implementation-readiness-audit/epic-04-data-hpc-verdict/report-016-hpc-correctness.md` section 6 summary table.

- **Verdict synthesis by threshold application**: the readiness verdict (report-017) applies three explicit threshold criteria rather than qualitative synthesis. The GO threshold (zero Resolve-Before-Coding gaps, zero NOT READY phases, all testing specs ADEQUATE) is binary. The NO-GO threshold (any gap without a concrete resolution path, or total effort >20 days) is a safety bound. Everything in between is CONDITIONAL GO. Applying thresholds mechanically ensures the verdict is reproducible from the evidence. See `plans/implementation-readiness-audit/epic-04-data-hpc-verdict/report-017-readiness-verdict.md` section 1 for the three-criterion justification structure.

- **Conditions inventory as the operative artifact of a CONDITIONAL GO verdict**: for a CONDITIONAL GO outcome, the conditions inventory table (report-017 section 2) is more actionable than the verdict statement itself. Each of the 18 conditions specifies: condition ID, description, phase(s) blocked, effort, timing batch, and dependencies. The inventory is structured to enable direct scheduling without further analysis. Future audit plans that expect a CONDITIONAL GO should treat the conditions inventory as the primary deliverable.

## Architectural Decisions

- **CONDITIONAL GO with 18 enumerated conditions and a 9-15 day pre-coding envelope**: the final verdict for the entire Cobre specification corpus is CONDITIONAL GO. The verdict is not a close call -- all 16 reports converge and the two Epic 04 reports introduced zero new gap identifiers, confirming that no latent architectural gaps remain. The 9-15 working day estimate is well under the 20-day NO-GO threshold. The decisive evidence is: 3 Resolve-Before-Coding gaps with concrete resolution paths (GAP-020 2-3 days, GAP-023 1-2 days, GAP-029 2-3 days), 1 NOT READY phase (Phase 7) with a concrete resolution window (during Phase 6), and 16 reports with no mutual contradictions. See `plans/implementation-readiness-audit/epic-04-data-hpc-verdict/report-017-readiness-verdict.md` section 1 justification paragraph.

- **Two-batch resolution schedule without delaying Phases 1-4**: the conditions inventory organizes into exactly 2 batches. Batch 1 (16 conditions, 7-12 days) has no dependencies on implementation and can run in parallel with Phase 1-4 coding. Batch 2 (2 conditions: C-13 GAP-020 + C-14 simulation types, 2-3 days) is explicitly deferred to during Phase 6 coding because the training pipeline patterns inform the simulation API design. This two-batch structure was recommended in the Epic 03 learnings and confirmed by the data traceability findings (output chains all blocked by the same root cause). Phases 1-4 can begin immediately after Batch 1 workstreams for Phases 1-3 are complete. See report-017 sections 2 and 3.

- **Input side is substantially more complete than output side due to anchoring function**: the `load_case(path: &Path) -> Result<System, LoadError>` entry point in `input-loading-pipeline.md` SS8.1 anchors all 4 input chains. An equivalent anchoring function for the output side does not exist. Report-015 section 5, Finding 3 states that specifying a single `create_output_context(...)` or output writer trait entry point would cascade improvements across all 4 output chains simultaneously. The specification of GAP-020 should lead with this anchoring function rather than designing 9 API elements independently.

- **HPC spec corpus is SUFFICIENT WITH GAPS with zero blocking findings**: all 4 HPC correctness dimensions received SUFFICIENT or SUFFICIENT WITH GAPS verdicts (report-016 section 6). Zero blocking findings were identified across 15 HPC spec files and 3 architecture files. The `i32`/`usize` type mismatch in `work-distribution.md` SS3.2 (F-016-001) is the only finding that could cause implementer confusion; the conversion is documented in `backend-ferrompi.md` SS1.2 but the per-site type annotation is inconsistent with the Communicator trait. All other HPC findings are Low severity stale name references. HPC specs can be used as implementation reference immediately after the mechanical naming fixes in Batch 1 (C-08, C-11, C-12).

- **Epic 04 findings confirmed existing gap inventory without adding new entries**: report-015 section 5, Finding 5 explicitly states that all data traceability findings map to existing GAP-020, GAP-023, or GAP-030 -- no new gap identifiers were needed. Report-016 introduced 7 findings but all are Medium or Low severity and map to existing conditions or were already captured as sub-items of C-08 (stale `split_shared_memory()` names). This convergence across independently scoped audits increases confidence that the conditions inventory in report-017 is complete.

## Files and Structures Created

- `plans/implementation-readiness-audit/epic-04-data-hpc-verdict/report-015-data-traceability.md` -- Data model end-to-end traceability report (5 sections). Traces 8 data format chains (4 input, 4 output) from source schema through pipeline function to in-memory or on-disk representation. Verdicts: 3 COMPLETE (JSON entity registries, JSON config files, Parquet scenario files), 2 PARTIAL (FlatBuffers policy input, FlatBuffers policy output), 3 SCHEMA ONLY (simulation results, training convergence/timing, metadata/manifest/dictionaries). All output chain gaps map to GAP-020 and GAP-030.

- `plans/implementation-readiness-audit/epic-04-data-hpc-verdict/report-016-hpc-correctness.md` -- HPC spec correctness audit (6 sections). Covers 15 HPC files and 3 architecture files across 4 dimensions. 7 findings (0 High, 3 Medium, 4 Low, 0 Blocking). Key findings: `i32`/`usize` allgatherv type mismatch in `work-distribution.md` SS3.2 (F-016-001, Medium); stale rayon reference in `training-loop.md` SS4.3 (F-016-003, Medium); 4 stale `split_shared_memory()` references in `communicator-trait.md` and `communication-patterns.md` (F-016-006, F-016-007, Low, confirmed from report-008 F-003). One new stale reference discovered in `communication-patterns.md` line 208 beyond the scope of report-008.

- `plans/implementation-readiness-audit/epic-04-data-hpc-verdict/report-017-readiness-verdict.md` -- Final readiness verdict report (7 sections). CONDITIONAL GO determination with 18 conditions totaling 9-15 working days. Includes: conditions inventory table with all 6 required fields per condition; two-batch resolution schedule; consolidated 8-row phase readiness table; 8-row crate readiness ranking; top 3 risk assessment (Phase 6 multi-crate integration, GAP-020 resolution quality, convention violation navigability); audit coverage summary (16 reports, 503 items, 130 findings, 4,689 links, 247 tests).

## Conventions Adopted

- **Chain completeness table format**: the 8-row chain summary table in report-015 section 4 uses five columns (chain ID, direction, source, sink, verdict, blocking gap IDs). The blocking gap IDs column is the key addition over a simple completeness table -- it links each non-COMPLETE chain directly to the gap inventory, enabling resolution scheduling without re-reading the chain assessment narrative.

- **HPC dimension verdict three-level scale**: SUFFICIENT (no implementation blockers, minimal friction), SUFFICIENT WITH GAPS (specific findings that should be corrected before implementation), INSUFFICIENT (would block correct implementation). This scale is more discriminating than PASS/FAIL for correctness dimensions where partial correctness is common. The scale should be reused in any future correctness audit that is not a completeness audit.

- **Verdict justification with three-criterion structure**: the CONDITIONAL GO justification in report-017 section 1 follows a three-paragraph structure: (1) why NOT GO (list the blocking criteria that fail the GO threshold), (2) why NOT NO-GO (cite the bounding evidence: concrete resolution paths and bounded effort), (3) why CONDITIONAL GO (the synthesis). This structure forces the verdict author to explicitly test all three outcomes rather than anchoring on the expected result.

- **Crate readiness ranking by three combined dimensions**: report-017 section 5 ranks crates by combining completeness percentage (from Epic 01), gap severity (from report-009), and phase verdict (from reports 012-013). The three-dimensional ranking avoids the pitfall of ranking by completeness alone (ferrompi 96% vs cobre-comm 77%, but Phase 4 is the only READY phase in Phases 1-4). The ranking explicitly notes the inverse correlation with "behavior first, types later" authoring pattern intensity.

- **Audit coverage summary with aggregate statistics**: report-017 section 7 documents 8 orthogonal audit dimensions covered by the 16 reports, a per-report scope table, and aggregate statistics (503 items, 130 findings, 4,689 links, 247 tests). This coverage summary establishes the evidentiary basis for confidence in the verdict and should be included in any multi-epic audit plan's terminal report.

## Surprises and Deviations

- **Zero new gap identifiers from two new audit dimensions**: the expectation entering Epic 04 was that data model traceability and HPC correctness would surface previously unknown gaps. In practice, all Epic 04 findings mapped to existing gap IDs (GAP-020, GAP-023, GAP-030) or were sub-items of conditions already identified in Epics 01-03. The `split_shared_memory()` stale reference in `communication-patterns.md` (F-016-006) was new in the sense that report-008 had not captured it, but it is a sub-instance of the same naming pattern already tracked. This convergence is evidence that the earlier audits were thorough, not that Epic 04 audited superficially.

- **The FlatBuffers chain is the strongest output chain despite a PARTIAL verdict**: the FlatBuffers policy chain (chain 7 of 8) received PARTIAL rather than SCHEMA ONLY because the `.fbs` schema is machine-readable and fully verified (all 8 tables COMPLETE per report-002), the cut pool memory layout is specified at the requirements level with performance rationale, and the checkpoint protocol documents reproducibility requirements and execution modes. The gap is a function signature and an in-memory struct -- both constrained by existing specifications. This chain can be resolved with less design ambiguity than the Parquet simulation chain (chain 5), which is missing both the in-memory source type and the writer function signature.

- **NUMA/cache guidance is predominantly ACTIONABLE -- more than expected**: the audit expectation was that NUMA/cache guidance would be aspirational (general principles without implementation-level detail). In practice, 12 of 15 NUMA/cache guidelines in the HPC specs are ACTIONABLE: they specify exact alignment values (64 bytes), specific code paths (NUMA initialization sequence in `memory-architecture.md` SS3.2), and specific data structures (cut pool component layout in `cut-management-impl.md` SS1.1). Only 3 guidelines are ASPIRATIONAL (deployment recommendation, latency reference table, workspace field ordering). See `report-016-hpc-correctness.md` section 4.

- **threading model inconsistency is confined to architecture files, not HPC files**: the two rayon references that conflict with the approved OpenMP-via-C-FFI threading model appear in `training-loop.md` SS4.3 and `solver-workspaces.md` SS1.3a -- both architecture files authored before GAP-018 resolution. All 15 HPC spec files use OpenMP terminology consistently. This geographic containment (inconsistency in architecture files, consistency in HPC files) reflects the sequencing of the spec-readiness plan: HPC specs were finalized after the threading model decision; architecture specs predated it and were not retroactively updated. The fixes are single-sentence corrections with no structural implications.

- **18 conditions in the final verdict, 3 more than the 15 pre-conditions estimated in Epic 03**: the Epic 03 learnings estimated 15 pre-coding conditions across 8 phases. The final count is 18 because Epic 04 added 3 conditions: C-11 (rayon reference in `training-loop.md` SS4.3, from report-016 F-016-003), C-12 (allgatherv type mismatch in `work-distribution.md` SS3.2, from report-016 F-016-001), and C-14 was separated from C-13 (simulation types as a distinct condition from the output writer API). None of the 3 additions change any phase verdict or increase the total effort estimate materially -- all 3 are hour-scale fixes.

## Aggregate Statistics

- Tickets in epic: 3 (ticket-015, ticket-016, ticket-017)
- Reports produced: 3 (reports 015, 016, 017)
- Data format chains traced (report-015): 8 (3 COMPLETE, 2 PARTIAL, 3 SCHEMA ONLY)
- Input chains: 4 (3 COMPLETE, 1 PARTIAL)
- Output chains: 4 (0 COMPLETE, 2 PARTIAL, 3 SCHEMA ONLY -- one chain is PARTIAL)
- Findings in report-015: 5 (2 high-priority, 1 pattern observation, 1 strength finding, 1 administrative)
- HPC spec files audited (report-016): 18 (15 HPC + 3 architecture)
- HPC dimensions: 4 (SUFFICIENT WITH GAPS, SUFFICIENT WITH GAPS, SUFFICIENT, SUFFICIENT WITH GAPS)
- Findings in report-016: 7 (0 High, 3 Medium, 4 Low, 0 Blocking)
- Final verdict (report-017): CONDITIONAL GO
- Total conditions: 18 (16 Batch 1, 2 Batch 2)
- Total pre-coding effort: 9-15 working days (Batch 1: 7-12 days; Batch 2: 2-3 days)
- Audit coverage (all 16 reports): 503 completeness items, 130 findings, 4,689 links verified, 247 named tests

## Recommendations for Future Implementation Plan

- **Begin Batch 1 immediately in parallel with Phase 1-4 coding**: the 16 Batch 1 conditions are pure spec authoring tasks with no implementation prerequisites. The cobre-core entity struct workstream (C-01, C-02, C-03) and the validation checklist workstream (C-04) can run simultaneously. With two spec authors working in parallel, Batch 1 calendar time is 4-6 working days. Phases 1-4 coding can begin on C-01 completion (entity struct definitions anchor all downstream Rust types).

- **Define `SimulationScenarioResult` first when resolving GAP-020 in Batch 2**: report-015 section 3.1 confirms that the simulation result type is the missing link that prevents all other simulation chain links from being specified. Report-017 section 6, Risk 2 recommends starting GAP-020 resolution with the metadata/manifest/dictionary chain (simplest serialization requirements) to establish the writer trait pattern, then moving to `SimulationScenarioResult` to constrain the Parquet and FlatBuffers writer signatures. C-14 is listed as an ingredient of C-13, but the resolution order should be C-14 first.

- **Use `load_case` in `input-loading-pipeline.md` SS8.1 as the design template for the output writer API**: the input anchoring function (full entry point signature, error type, responsibility boundary) is the spec artifact that makes all 4 input chains traceable. The output writer API should be designed to the same standard: one anchoring entry point with a complete Rust signature, a concrete error type, and explicit responsibility boundaries between cobre-sddp (producer) and cobre-io (consumer). Report-015 section 5, Finding 3 identifies this as the highest-leverage single spec addition.

- **Schedule a convention cleanup pass before Phase 6 coding**: the 100 section-prefix convention violations (83 in architecture files, 17 in overview files) and 4 stale `split_shared_memory()` references create navigation friction for implementers cross-referencing specs during Phase 6. Phase 6 (cobre-sddp training) has the highest spec reading load of any phase -- 14 spec files in its reading list. A dedicated batch `ยง`-to-`SS` replacement pass (report-017 section 6, Risk 3 mitigation) should be scheduled as a standalone low-effort ticket before Phase 6 begins.

- **Use the 3-bus integration test as the Phase 6 quality gate**: report-017 section 6, Risk 1 identifies the Phase 6 multi-crate integration as the highest-risk implementation event. The `implementation-ordering.md` section 5 specifies a 3-bus, 2-hydro, 1-thermal, 12-stage, 4-MPI-rank integration test as the Phase 6 validation point. This test should be the acceptance criterion for Phase 6 completion, not individual unit tests. The integration test surface covers all 5 upstream crate APIs simultaneously and is the most sensitive detector of API assumption mismatches.
